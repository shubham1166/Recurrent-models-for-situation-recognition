{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fusion_Network_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFNbNves7dp",
        "colab_type": "text"
      },
      "source": [
        "# Shubham Sharma\n",
        "## IIT Bombay\n",
        "This code is for predicting verbs using fusion network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GL7oWXOMERS",
        "colab_type": "code",
        "outputId": "6b39a20d-26a1-456f-c914-408ad24fb2dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6s8luwLs4rZ",
        "colab_type": "code",
        "outputId": "220393f3-65bd-446b-fb83-7efa3357a536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# For plotting, arrays and other things\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#For Deep learning \n",
        "import keras\n",
        "from keras import callbacks \n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input\n",
        "from keras.optimizers import RMSprop,SGD, adam #Optimizers\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kJ0hoKg3PE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fusion_network(width, height, depth, classes,weightsPath=None):\n",
        "  \n",
        "  bn_flag = True\n",
        "  inputs = Input((width, height , depth))\n",
        "  conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "  conv1 = BatchNormalization(axis = -1)(conv1,training=bn_flag)\n",
        "  conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "  conv1 = BatchNormalization(axis= -1)(conv1,training=bn_flag)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  ##\n",
        "  conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
        "  conv2 = BatchNormalization(axis = -1)(conv2,training=bn_flag)\n",
        "  conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
        "  conv2 = BatchNormalization(axis= -1)(conv2,training=bn_flag)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  ##\n",
        "  conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
        "  conv3 = BatchNormalization(axis = -1)(conv3,training=bn_flag)\n",
        "  conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
        "  conv3 = BatchNormalization(axis = -1)(conv3,training=bn_flag)\n",
        "  conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
        "  conv3 = BatchNormalization(axis = -1)(conv3,training=bn_flag)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  ##\n",
        "  conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "  conv4 = BatchNormalization(axis = -1)(conv4,training=bn_flag)\n",
        "  conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "  conv4 = BatchNormalization(axis = -1)(conv4,training=bn_flag)\n",
        "  conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "  conv4 = BatchNormalization(axis = -1)(conv4,training=bn_flag)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "  ##\n",
        "  conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "  conv5 = BatchNormalization(axis = -1)(conv5,training=bn_flag)\n",
        "  conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "  conv5 = BatchNormalization(axis = -1)(conv5,training=bn_flag)\n",
        "  conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "  conv5 = BatchNormalization(axis = -1)(conv5,training=bn_flag)\n",
        "  pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
        "  ##\n",
        "  Y = Flatten()(pool5)\n",
        "  Y = Dense(4096, activation='relu')(Y)\n",
        "  Y = Dense(4096, activation='relu')(Y)\n",
        "  Y = Dense(4096, activation='relu')(Y)\n",
        "  out = Dense(classes, activation='softmax')(Y)\n",
        "  \n",
        "  model = Model(inputs=[inputs], outputs=[out])\n",
        "  \n",
        "  if weightsPath is not None:\n",
        "    model.load_weights(weightsPath)\n",
        "    \n",
        "  print(model.summary())\n",
        "  return model \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBwLzPBnL_RK",
        "colab_type": "text"
      },
      "source": [
        "Loading and pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KTQwHpYMCTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = 50 \n",
        "path2load = 'gdrive/My Drive/Situation_recognition/data/'\n",
        "\n",
        "X_name = np.load(path2load + 'X_name.npy')\n",
        "Y_verbs = np.load(path2load + 'Y_verbs.npy')\n",
        "dictionary_fifty_verbs = np.load(path2load + 'dictionary_fifty_verbs.npy')\n",
        "X = np.load(path2load + 'X.npy')\n",
        "Y = to_categorical(Y_verbs, classes)\n",
        "\n",
        "# x_train = np.load(path2load + 'x_train.npy')\n",
        "# x_test = np.load(path2load + 'x_test.npy')\n",
        "# y_train = np.load(path2load + 'y_train.npy')\n",
        "# y_test = np.load(path2load + 'y_test.npy')\n",
        "# y_test = to_categorical(y_test, classes)\n",
        "# y_train = to_categorical(y_train, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gJ9gpV4JDIZ",
        "colab_type": "code",
        "outputId": "09cf56df-b36a-43d8-c279-96394e479880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "path2save = 'gdrive/My Drive/Situation_recognition/fusion'\n",
        "#Define Image Size and Number of classes\n",
        "width, height, depth, classes = 256 , 256 , 3 , 50 \n",
        "model = fusion_network(width, height, depth, classes , weightsPath='gdrive/My Drive/Situation_recognition/fusion')\n",
        "print (\"Compiling Model...\")\n",
        "#Store the network weights whenever loss is minimum than previous epoch\n",
        "# modelCheck = callbacks.ModelCheckpoint(path2save+'_{epoch:04d}-{acc:.4f}.h5', monitor='loss', mode='auto')\n",
        "modelCheck = callbacks.ModelCheckpoint(path2save, monitor='loss', verbose=0, mode='auto')\n",
        " \n",
        "opt = adam(lr=1e-5)\n",
        "#Set the compiler parameter for the training\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"categorical_accuracy\"],sample_weight_mode='auto')\n",
        "print (\"Training the Model...\")\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 256, 256, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 128, 128, 128)     512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 64, 64, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16, 16, 512)       2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4096)              134221824 \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                204850    \n",
            "=================================================================\n",
            "Total params: 182,720,882\n",
            "Trainable params: 182,712,434\n",
            "Non-trainable params: 8,448\n",
            "_________________________________________________________________\n",
            "None\n",
            "Compiling Model...\n",
            "Training the Model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxoVONxjnkOT",
        "colab_type": "code",
        "outputId": "871b4e27-8709-413d-82e8-e80a266d433a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "#Train the Network\n",
        "# history = model.fit(X, Y, batch_size = 4, epochs= 10, verbose=1, callbacks= [modelCheck], shuffle = True, validation_split = O.2)#,validation_data = (x_test , y_test)\n",
        "history = model.fit(X , Y, batch_size=4, epochs=30, verbose=1, callbacks= [modelCheck], validation_split=0.2, shuffle=True)#,validation_data = (x_test , y_test)\n",
        "print (\"Dumping Weights to file...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10703 samples, validate on 2676 samples\n",
            "Epoch 1/30\n",
            "10703/10703 [==============================] - 1376s 129ms/step - loss: 0.0803 - categorical_accuracy: 0.9820 - val_loss: 4.5530 - val_categorical_accuracy: 0.3188\n",
            "Epoch 2/30\n",
            "10703/10703 [==============================] - 1370s 128ms/step - loss: 0.0163 - categorical_accuracy: 0.9972 - val_loss: 5.0573 - val_categorical_accuracy: 0.3173\n",
            "Epoch 3/30\n",
            "  156/10703 [..............................] - ETA: 21:23 - loss: 3.1505e-04 - categorical_accuracy: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5f35e34e396d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodelCheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,validation_data = (x_test , y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Dumping Weights to file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Dumping Weights to file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ELe_SrJRNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path2save = 'gdrive/My Drive/Situation_recognition/fusion'\n",
        "# #Define Image Size and Number of classes\n",
        "# width, height, depth, classes = 256 , 256 , 3 , 50 \n",
        "# model = fusion_network(width, height, depth, classes , weightsPath=None)\n",
        "# print (\"Compiling Model...\")\n",
        "# #Store the network weights whenever loss is minimum than previous epoch\n",
        "# modelCheck = callbacks.ModelCheckpoint(path2save+'.h5', monitor='acc', mode='auto')\n",
        "# # modelCheck = callbacks.ModelCheckpoint(path2save, monitor='acc', verbose=0, save_best_only=True, mode='auto')\n",
        " \n",
        "# opt = adam(lr=1e-4)\n",
        "# #Set the compiler parameter for the training\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"categorical_accuracy\"],sample_weight_mode='auto')\n",
        "# print (\"Training the Model...\")\n",
        "\n",
        "# train_datagen = ImageDataGenerator(rescale=1./255)#, validation_split=0.2, interpolation_order=1, dtype='float32')\n",
        "# # test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# train_datagen.fit(X)\n",
        "# # fits the model on batches with real-time data augmentation:\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}